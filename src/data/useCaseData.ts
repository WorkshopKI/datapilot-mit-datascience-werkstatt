// Use Case Library Data
// 12 vollst√§ndige Use Cases f√ºr die Use-Case-Bibliothek

export interface DataSource {
  source: string;
  features: string[];
}

export interface Pitfall {
  title: string;
  problem: string;
  whyBad: string;
  solution: string;
}

export interface PracticeStory {
  title: string;
  situation: string;
  problem: string;
  cause: string;
  learning: string;
}

export interface ModelMetric {
  metric: string;
  explanation: string;
}

export interface UseCase {
  id: string;
  emoji: string;
  title: string;
  industry: string;
  problemType: string;
  level: "beginner" | "intermediate" | "expert";
  levelStars: number;
  shortDescription: string;
  
  // √úbersicht
  goal: string;
  decision: string;
  intervention: string;
  baseline: string;
  
  // Kennzahlen
  businessKPIs: string[];
  modelMetrics: ModelMetric[];
  metricsNote: string;
  
  // Typische Daten
  dataSources: DataSource[];
  labelDefinition: string;
  
  // Stolperfallen
  pitfalls: Pitfall[];
  
  // Praxis-Story
  practiceStory: PracticeStory;
}

export const problemTypes = [
  { id: "churn", label: "Abwanderung (Churn)" },
  { id: "conversion", label: "Conversion & Upselling" },
  { id: "risk", label: "Risiko & Anomalie" },
  { id: "demand", label: "Nachfrage & Menge" },
  { id: "maintenance", label: "Ausfall & Wartung" },
];

export const industries = [
  { id: "telco", label: "Telekommunikation" },
  { id: "saas", label: "SaaS" },
  { id: "bank", label: "Bank / Finanzwesen" },
  { id: "ecommerce", label: "E-Commerce" },
  { id: "retail", label: "Einzelhandel" },
  { id: "insurance", label: "Versicherung" },
  { id: "fintech", label: "Fintech" },
  { id: "manufacturing", label: "Fertigung" },
];

export const levels = [
  { id: "beginner", label: "Einsteiger", stars: 1 },
  { id: "intermediate", label: "Fortgeschritten", stars: 2 },
  { id: "expert", label: "Experte", stars: 3 },
];

export const useCases: UseCase[] = [
  // ========== CHURN (3) ==========
  {
    id: "churn-telco",
    emoji: "üì±",
    title: "Churn Prediction",
    industry: "telco",
    problemType: "churn",
    level: "beginner",
    levelStars: 1,
    shortDescription: "Identifiziere Kunden, die ihren Mobilfunkvertrag k√ºndigen werden, bevor sie es tun.",
    
    goal: "Kunden mit hohem K√ºndigungsrisiko fr√ºhzeitig erkennen und durch gezielte Ma√ünahmen halten.",
    decision: "Welche Kunden sollen vom Retention-Team kontaktiert werden?",
    intervention: "Proaktiver Anruf mit personalisiertem Angebot (z.B. Rabatt, Upgrade, Zusatzleistung).",
    baseline: "Aktuell werden Kunden zuf√§llig oder nach Bauchgef√ºhl kontaktiert ‚Äì Erfolgsquote: ~15%.",
    
    businessKPIs: [
      "Churn-Rate (monatlich/j√§hrlich)",
      "Customer Lifetime Value (CLV)",
      "Retention-Rate nach Intervention",
      "Kosten pro gerettetem Kunden",
    ],
    modelMetrics: [
      { metric: "Precision", explanation: "Wie viele der als Churn-Risiko markierten Kunden wollen wirklich k√ºndigen? Wichtig bei begrenzter Anrufkapazit√§t." },
      { metric: "Recall", explanation: "Wie viele der tats√§chlichen K√ºndiger wurden erkannt? Wichtig, wenn jeder verlorene Kunde teuer ist." },
      { metric: "AUC-ROC", explanation: "Gesamtqualit√§t der Risiko-Ranking ‚Äì wie gut trennt das Modell K√ºndiger von Bleibern?" },
    ],
    metricsNote: "Bei begrenzter Team-Kapazit√§t ist Precision wichtiger als Recall. Lieber weniger, aber die richtigen Kunden anrufen.",
    
    dataSources: [
      { source: "CRM-System", features: ["Vertragslaufzeit", "Tarif", "K√ºndigungen in Vergangenheit", "Beschwerden"] },
      { source: "Billing-System", features: ["Monatliche Kosten", "Zahlungshistorie", "Rechnungsreklamationen"] },
      { source: "Nutzungsdaten", features: ["Datenverbrauch", "Anrufminuten", "App-Nutzung", "Roaming"] },
      { source: "Kundenservice", features: ["Ticket-Anzahl", "Beschwerdekategorien", "NPS-Score"] },
    ],
    labelDefinition: "Churn = Kunde hat innerhalb von 30 Tagen nach Scoring gek√ºndigt oder portiert.",
    
    pitfalls: [
      {
        title: "Label-Definition zu sp√§t",
        problem: "Churn wird erst definiert, wenn der Kunde bereits gek√ºndigt hat.",
        whyBad: "Dann ist die Intervention zu sp√§t ‚Äì der Kunde ist mental schon weg.",
        solution: "Churn 30-60 Tage vorher definieren, um Handlungsspielraum zu haben.",
      },
      {
        title: "Vertragslaufzeit ignoriert",
        problem: "Kunden mit 24-Monats-Vertrag k√∂nnen gar nicht k√ºndigen.",
        whyBad: "Modell lernt irrelevante Muster, wenn Bindungsfrist nicht ber√ºcksichtigt wird.",
        solution: "Nur Kunden in den letzten 3 Monaten der Vertragslaufzeit betrachten.",
      },
      {
        title: "Saisonale Effekte √ºbersehen",
        problem: "Training nur auf Sommerdaten, Einsatz im Winter.",
        whyBad: "Weihnachtsgesch√§ft und Wechselsaison haben andere Muster.",
        solution: "Mindestens 12 Monate Trainingsdaten verwenden.",
      },
      {
        title: "Intervention nicht messbar",
        problem: "Alle Top-Risiko-Kunden werden angerufen.",
        whyBad: "Ohne Kontrollgruppe kann man den Effekt nicht messen.",
        solution: "10% Holdout-Gruppe nicht kontaktieren, um Uplift zu messen.",
      },
      {
        title: "Kosten der Retention ignoriert",
        problem: "Jeder Kunde mit Churn-Risiko bekommt teures Angebot.",
        whyBad: "Manche Kunden w√§ren auch ohne Rabatt geblieben ‚Äì verschenktes Geld.",
        solution: "Uplift-Modelling: Nur Kunden mit positivem Response auf Intervention targeten.",
      },
    ],
    
    practiceStory: {
      title: "Der teure Anruf",
      situation: "Ein Telekommunikationsanbieter implementierte ein Churn-Modell mit 85% Accuracy und feierte den Erfolg.",
      problem: "Nach 6 Monaten zeigte sich: Die Retention-Rate war nicht gestiegen, aber die Kosten f√ºr Rabatte explodierten.",
      cause: "Das Modell hatte hohen Recall, aber niedrige Precision. Viele kontaktierte Kunden hatten nie vor zu k√ºndigen ‚Äì nahmen aber gerne den Rabatt mit.",
      learning: "Bei begrenzten Ressourcen ist Precision wichtiger als Recall. Au√üerdem fehlte eine Kontrollgruppe, um den echten Uplift zu messen.",
    },
  },
  
  {
    id: "churn-saas",
    emoji: "‚òÅÔ∏è",
    title: "Subscription Churn",
    industry: "saas",
    problemType: "churn",
    level: "intermediate",
    levelStars: 2,
    shortDescription: "Erkenne SaaS-Kunden, die ihr Abo nicht verl√§ngern werden, anhand von Nutzungsmustern.",
    
    goal: "Abwanderungsgef√§hrdete B2B-Kunden identifizieren und durch Customer Success reaktivieren.",
    decision: "Welche Accounts brauchen proaktive Betreuung durch Customer Success Manager?",
    intervention: "Pers√∂nlicher Check-in, Feature-Training, Eskalation an Entscheider.",
    baseline: "CSMs betreuen Accounts nach Umsatzgr√∂√üe, nicht nach Risiko ‚Äì viele kleinere Churner werden √ºbersehen.",
    
    businessKPIs: [
      "Monthly Recurring Revenue (MRR) Churn",
      "Net Revenue Retention (NRR)",
      "Customer Health Score",
      "Time-to-Value f√ºr Neukunden",
    ],
    modelMetrics: [
      { metric: "Precision@k", explanation: "Von den Top-k Risiko-Accounts: Wie viele churnen wirklich? Wichtig bei limitierter CSM-Kapazit√§t." },
      { metric: "Lead Time", explanation: "Wie fr√ºh vor dem Renewal warnt das Modell? Mindestens 60 Tage n√∂tig f√ºr Intervention." },
      { metric: "Revenue-weighted Recall", explanation: "Werden die umsatzst√§rksten Churner erkannt?" },
    ],
    metricsNote: "B2B-Churn ist oft MRR-gewichtet ‚Äì ein Enterprise-Kunde z√§hlt mehr als 100 Starter-Accounts.",
    
    dataSources: [
      { source: "Produkt-Analytics", features: ["DAU/MAU", "Feature-Adoption", "Session-Dauer", "Aktivit√§ts-Trend"] },
      { source: "CRM (Salesforce)", features: ["Vertragswert", "Renewal-Datum", "Kontakthistorie", "Support-Tickets"] },
      { source: "Billing", features: ["Zahlungsverz√∂gerungen", "Downgrades", "Add-on-Nutzung"] },
      { source: "NPS/Surveys", features: ["NPS-Score", "CSAT nach Support", "Feature-Requests"] },
    ],
    labelDefinition: "Churn = Account hat Subscription nicht verl√§ngert oder auf Free-Tier downgraded.",
    
    pitfalls: [
      {
        title: "Seat-Reduktion ignoriert",
        problem: "Nur komplette Churner werden als Label verwendet.",
        whyBad: "Kunden, die von 50 auf 5 Seats reduzieren, sind genauso problematisch.",
        solution: "Contraction (>50% MRR-Verlust) als separates Label oder gemeinsam modellieren.",
      },
      {
        title: "Onboarding-Phase nicht separiert",
        problem: "Neue Accounts in den ersten 90 Tagen werden genauso behandelt.",
        whyBad: "Niedrige Nutzung in der Onboarding-Phase ist normal, nicht unbedingt Churn-Signal.",
        solution: "Separate Modelle oder Features f√ºr Onboarding-Accounts.",
      },
      {
        title: "Champion-Wechsel nicht erkannt",
        problem: "Modell basiert auf Account-Level-Daten.",
        whyBad: "Wenn der interne Champion das Unternehmen verl√§sst, steigt das Churn-Risiko drastisch.",
        solution: "User-Level-Activity und Kontakt-√Ñnderungen als Features einbauen.",
      },
      {
        title: "Renewal-Timing ignoriert",
        problem: "Churn-Score wird monatlich f√ºr alle Accounts berechnet.",
        whyBad: "Accounts mit Renewal in 6 Monaten brauchen andere Intervention als in 30 Tagen.",
        solution: "Time-to-Renewal als Feature und f√ºr Priorisierung nutzen.",
      },
      {
        title: "Produkt-Feedback nicht integriert",
        problem: "Nur Nutzungsdaten, keine qualitativen Signale.",
        whyBad: "Feature-Requests und Support-Eskalationen sind starke Churn-Pr√§diktoren.",
        solution: "Ticket-Sentiment und Feature-Request-Status als Features.",
      },
    ],
    
    practiceStory: {
      title: "Der stille Abgang",
      situation: "Ein SaaS-Unternehmen trainierte ein Churn-Modell auf Nutzungsdaten und erreichte gute Metriken.",
      problem: "Trotzdem churnte ein Enterprise-Kunde (500k ARR) √ºberraschend bei Renewal.",
      cause: "Der Account hatte weiterhin hohe Nutzung, aber der VP Engineering (Champion) hatte 3 Monate vorher das Unternehmen verlassen. Das Modell erkannte nur Account-Level-Aktivit√§t, nicht User-Level-√Ñnderungen.",
      learning: "Champion-Tracking und Kontakt-√Ñnderungen sind kritische Signale, die reine Nutzungsdaten nicht abbilden.",
    },
  },
  
  {
    id: "churn-bank",
    emoji: "üè¶",
    title: "Customer Attrition",
    industry: "bank",
    problemType: "churn",
    level: "intermediate",
    levelStars: 2,
    shortDescription: "Identifiziere Bankkunden, die ihr Konto aufl√∂sen oder zur Konkurrenz wechseln.",
    
    goal: "Profitable Kunden mit Abwanderungsrisiko erkennen und durch gezielte Angebote binden.",
    decision: "Welche Kunden sollen ein Retention-Angebot erhalten?",
    intervention: "Pers√∂nliche Beratung, Konditionsverbesserung, Cross-Selling relevanter Produkte.",
    baseline: "Kunden werden erst kontaktiert, wenn sie bereits die K√ºndigung eingereicht haben.",
    
    businessKPIs: [
      "Kundenabwanderungsrate (j√§hrlich)",
      "Customer Lifetime Value (CLV)",
      "Kosten pro Neukundenakquise vs. Retention",
      "Share of Wallet",
    ],
    modelMetrics: [
      { metric: "Precision", explanation: "Wie viele der als Risiko markierten Kunden wollten wirklich wechseln?" },
      { metric: "CLV-weighted Recall", explanation: "Werden die profitabelsten Kunden mit Churn-Risiko erkannt?" },
      { metric: "Decile Lift", explanation: "Um wieviel besser ist das Top-Dezil vs. Zufallsauswahl?" },
    ],
    metricsNote: "Nicht jeder Kunde ist gleich profitabel ‚Äì unprofitable Churner sind kein Verlust.",
    
    dataSources: [
      { source: "Kernbanksystem", features: ["Kontotypen", "Einlagen", "Kredite", "Produkt-Cross-Selling"] },
      { source: "Transaktionsdaten", features: ["Transaktionsvolumen", "Gehaltseing√§nge", "Dauerauftr√§ge-√Ñnderungen"] },
      { source: "Online-Banking", features: ["Login-Frequenz", "App-Nutzung", "Feature-Nutzung"] },
      { source: "Kundencenter", features: ["Beschwerden", "Konditionsanfragen", "Konkurrenz-Erw√§hnungen"] },
    ],
    labelDefinition: "Churn = Hauptkonto aufgel√∂st oder Gehaltseing√§nge auf 0 innerhalb von 90 Tagen.",
    
    pitfalls: [
      {
        title: "Inaktive Konten einbezogen",
        problem: "Alle Konten werden gleichbehandelt.",
        whyBad: "Viele 'Zombie-Konten' mit 5‚Ç¨ Guthaben verzerren das Modell.",
        solution: "Nur aktive Kunden (z.B. >3 Transaktionen/Monat) betrachten.",
      },
      {
        title: "Stille Abwanderung √ºbersehen",
        problem: "Churn = Kontoaufl√∂sung.",
        whyBad: "Viele Kunden l√∂sen nicht auf, sondern reduzieren nur die Nutzung.",
        solution: "Aktivit√§ts-basierte Churn-Definition: z.B. Gehalt nicht mehr eingehend.",
      },
      {
        title: "Regulatorische K√ºndigungen",
        problem: "Alle K√ºndigungen werden als Churn gez√§hlt.",
        whyBad: "Zwangsk√ºndigungen wegen Compliance sind nicht vorhersagbar.",
        solution: "Regulatorische Gr√ºnde als separates Label oder Filter.",
      },
      {
        title: "Produkt-Kannibalisierung",
        problem: "Kunde wechselt nur das Produkt, nicht die Bank.",
        whyBad: "Wechsel von Girokonto auf Tagesgeld ist kein echter Churn.",
        solution: "Kundenbeziehungs-Level betrachten, nicht einzelne Produkte.",
      },
      {
        title: "Datenschutz-Einschr√§nkungen",
        problem: "Wichtige Features sind aus Compliance-Gr√ºnden nicht nutzbar.",
        whyBad: "Modell hat weniger Vorhersagekraft als technisch m√∂glich.",
        solution: "Fr√ºhzeitig mit Legal/Compliance abstimmen, welche Daten nutzbar sind.",
      },
    ],
    
    practiceStory: {
      title: "Die Gehalts√ºberweisung",
      situation: "Eine Bank trainierte ein Churn-Modell auf Kontoaufl√∂sungen und erreichte 80% AUC.",
      problem: "Trotzdem sanken die Einlagen dramatisch, obwohl die Kontoaufl√∂sungen stabil blieben.",
      cause: "Kunden l√∂sten ihre Konten nicht auf, sondern lie√üen sie einfach liegen ‚Äì ihr Gehalt ging zur Konkurrenz. Das Modell erkannte nur formale K√ºndigungen, nicht die 'stille Abwanderung'.",
      learning: "Die Churn-Definition muss das echte Gesch√§ftsproblem abbilden. Kontoaufl√∂sung ‚â† Kundenabwanderung.",
    },
  },
  
  // ========== CONVERSION & UPSELLING (3) ==========
  {
    id: "upselling-ecommerce",
    emoji: "üõí",
    title: "Upselling Propensity",
    industry: "ecommerce",
    problemType: "conversion",
    level: "beginner",
    levelStars: 1,
    shortDescription: "Identifiziere Kunden mit hoher Wahrscheinlichkeit f√ºr Premium-Produkte oder gr√∂√üere Warenk√∂rbe.",
    
    goal: "Kunden identifizieren, die auf personalisierte Upgrade-Angebote positiv reagieren.",
    decision: "Welchen Kunden soll ein Premium-Upgrade oder Bundle angezeigt werden?",
    intervention: "Personalisierte Produktempfehlung, Bundle-Angebot, Premium-Version im Checkout.",
    baseline: "Alle Kunden sehen dieselben Upselling-Banner ‚Äì Conversion: ~2%.",
    
    businessKPIs: [
      "Average Order Value (AOV)",
      "Upsell-Conversion-Rate",
      "Revenue per Visitor",
      "Marge pro Upsell",
    ],
    modelMetrics: [
      { metric: "Precision", explanation: "Wie oft f√ºhrt das Upsell-Angebot zum Kauf? Wichtig f√ºr Kundenerlebnis." },
      { metric: "Lift vs. Random", explanation: "Wie viel besser ist das Modell als zuf√§llige Angebote?" },
      { metric: "Revenue Impact", explanation: "Zus√§tzlicher Umsatz durch das Modell vs. Baseline." },
    ],
    metricsNote: "A/B-Test ist essentiell ‚Äì ohne Vergleich kann man den echten Modell-Effekt nicht messen.",
    
    dataSources: [
      { source: "Shop-Analytics", features: ["Besuchte Kategorien", "Preissegment-Historie", "Verweildauer"] },
      { source: "Kaufhistorie", features: ["Durchschnittlicher Warenkorb", "Premium-K√§ufe", "Retourenquote"] },
      { source: "CRM", features: ["Kundensegment", "Newsletter-Engagement", "Loyalty-Status"] },
      { source: "Session-Daten", features: ["Device", "Referrer", "Zeit seit letztem Besuch"] },
    ],
    labelDefinition: "Positiv = Kunde hat Upsell-Angebot angenommen und nicht retourniert.",
    
    pitfalls: [
      {
        title: "Retouren ignoriert",
        problem: "Jeder Upsell-Kauf wird als Erfolg gez√§hlt.",
        whyBad: "Premium-Produkte haben oft h√∂here Retourenquoten.",
        solution: "Label erst nach Ablauf der Retourenfrist setzen.",
      },
      {
        title: "Kannibalisierung nicht gemessen",
        problem: "Upsell-Erfolge werden gez√§hlt, aber nicht Basis-Verk√§ufe.",
        whyBad: "Manche Kunden h√§tten auch ohne Upsell das Premium gekauft.",
        solution: "Kontrollgruppe ohne Upsell-Angebot f√ºr echten Uplift.",
      },
      {
        title: "Kundenerlebnis vernachl√§ssigt",
        problem: "Aggressive Upselling bei jedem Besuch.",
        whyBad: "Kann zu Kaufabbr√ºchen und Kundenver√§rgerung f√ºhren.",
        solution: "Frequenz-Capping und Confidence-Threshold f√ºr Angebote.",
      },
      {
        title: "Saisonalit√§t ignoriert",
        problem: "Modell trainiert auf Black-Friday-Daten.",
        whyBad: "Kaufverhalten in Sales-Phasen ist nicht repr√§sentativ.",
        solution: "Sales-Perioden separat modellieren oder ausschlie√üen.",
      },
      {
        title: "Margen nicht ber√ºcksichtigt",
        problem: "Upsell auf Produkte mit niedriger Marge.",
        whyBad: "H√∂herer Umsatz, aber nicht unbedingt mehr Gewinn.",
        solution: "Marge in die Zielgr√∂√üe oder Scoring einbeziehen.",
      },
    ],
    
    practiceStory: {
      title: "Der Retouren-Boom",
      situation: "Ein E-Commerce-Shop implementierte Upselling mit einem Modell, das 40% mehr Premium-K√§ufe erzielte.",
      problem: "Nach 3 Monaten zeigte sich: Die Retourenquote bei Premium-Produkten war 3x h√∂her als normal.",
      cause: "Das Modell hatte Kunden identifiziert, die gerne Premium 'ausprobieren' ‚Äì aber oft zur√ºckschicken. Die Conversion war hoch, der Nettogewinn negativ.",
      learning: "Das Label muss den echten Gesch√§ftserfolg abbilden: Kauf UND Behalten, nicht nur Kauf.",
    },
  },
  
  {
    id: "lead-scoring-b2b",
    emoji: "üéØ",
    title: "Lead Scoring",
    industry: "saas",
    problemType: "conversion",
    level: "intermediate",
    levelStars: 2,
    shortDescription: "Priorisiere B2B-Leads nach Abschlusswahrscheinlichkeit f√ºr effizienteren Vertrieb.",
    
    goal: "Sales-Team auf die vielversprechendsten Leads fokussieren und Conversion-Rate erh√∂hen.",
    decision: "Welche Leads sollen priorisiert von Sales kontaktiert werden?",
    intervention: "Priorisierte Bearbeitung durch Account Executives, personalisierte Outreach.",
    baseline: "Leads werden nach FIFO oder manuellem Scoring bearbeitet ‚Äì viele Cold Leads verschwenden Zeit.",
    
    businessKPIs: [
      "Lead-to-Opportunity Conversion",
      "Sales Cycle Length",
      "Revenue per Sales Rep",
      "Cost per Acquisition (CPA)",
    ],
    modelMetrics: [
      { metric: "Precision@k", explanation: "Von den Top-k Leads: Wie viele werden zu Opportunities?" },
      { metric: "Lead Time", explanation: "Wie fr√ºh im Funnel kann das Modell gute Leads erkennen?" },
      { metric: "AUC-PR", explanation: "Wichtiger als AUC-ROC bei stark unbalancierten Klassen (wenige Conversions)." },
    ],
    metricsNote: "B2B-Conversions sind selten ‚Äì Precision-Recall-Kurve ist aussagekr√§ftiger als Accuracy.",
    
    dataSources: [
      { source: "Marketing Automation", features: ["Website-Besuche", "Content-Downloads", "Email-Engagement"] },
      { source: "CRM (Salesforce)", features: ["Firmengr√∂√üe", "Branche", "Budget", "Entscheider-Kontakt"] },
      { source: "Enrichment (Clearbit)", features: ["Technografische Daten", "Wachstumssignale", "Funding"] },
      { source: "Intent Data", features: ["Themen-Recherche", "Konkurrenz-Vergleiche", "G2-Reviews"] },
    ],
    labelDefinition: "Conversion = Lead wurde zu qualifizierter Opportunity (SQL) innerhalb von 90 Tagen.",
    
    pitfalls: [
      {
        title: "Survivorship Bias",
        problem: "Training nur auf bearbeiteten Leads.",
        whyBad: "Nicht kontaktierte Leads fehlen ‚Äì vielleicht w√§ren sie konvertiert.",
        solution: "Stichprobe aller Leads bearbeiten, auch niedrig-scorende.",
      },
      {
        title: "Sales-Aktivit√§t als Feature",
        problem: "Anzahl Sales-Touches als Input f√ºr das Modell.",
        whyBad: "Kausalit√§t umgekehrt: Sales kontaktiert gute Leads √∂fter.",
        solution: "Nur Daten VOR erstem Sales-Kontakt als Features.",
      },
      {
        title: "Firmengr√∂√üe dominiert",
        problem: "Enterprise-Leads werden immer hoch gescoret.",
        whyBad: "Kleine Firmen mit hohem Fit werden √ºbersehen.",
        solution: "Segmentspezifische Modelle oder Gr√∂√üen-Normalisierung.",
      },
      {
        title: "Veraltete Daten",
        problem: "Enrichment-Daten sind Monate alt.",
        whyBad: "Funding-Runden, Entlassungen, etc. √§ndern das Potenzial.",
        solution: "Regelm√§√üiges Enrichment-Update, Decay f√ºr alte Daten.",
      },
      {
        title: "ICP nicht definiert",
        problem: "Modell optimiert auf alle Conversions.",
        whyBad: "Nicht alle Kunden sind gleich profitabel oder gut passend.",
        solution: "Ideal Customer Profile definieren und als Label-Filter nutzen.",
      },
    ],
    
    practiceStory: {
      title: "Die vergessenen Leads",
      situation: "Ein SaaS-Unternehmen trainierte Lead Scoring auf historischen Conversions und priorisierte danach.",
      problem: "Das Modell bevorzugte immer Enterprise-Leads ‚Äì SMB-Segment wurde vernachl√§ssigt.",
      cause: "Historisch hatte Sales Enterprise priorisiert ‚Üí mehr Conversions ‚Üí Modell lernte diesen Bias. SMB-Leads hatten nie eine Chance, da sie nicht kontaktiert wurden.",
      learning: "Training-Daten reflektieren vergangene Entscheidungen. Ohne Exploration (alle Leads mal kontaktieren) verst√§rkt das Modell bestehende Biases.",
    },
  },
  
  {
    id: "next-best-offer",
    emoji: "üéÅ",
    title: "Next Best Offer",
    industry: "retail",
    problemType: "conversion",
    level: "expert",
    levelStars: 3,
    shortDescription: "Empfehle jedem Kunden das individuell passendste Produkt oder Angebot.",
    
    goal: "Personalisierte Produktempfehlungen zur Steigerung von Conversion und Warenkorb.",
    decision: "Welches Produkt/Angebot soll diesem Kunden jetzt angezeigt werden?",
    intervention: "Personalisierte Banner, Email-Kampagnen, App-Notifications.",
    baseline: "Alle Kunden sehen dieselben Top-Seller oder Kategorie-basierte Empfehlungen.",
    
    businessKPIs: [
      "Click-Through-Rate (CTR) auf Empfehlungen",
      "Conversion-Rate der Empfehlungen",
      "Incremental Revenue",
      "Customer Engagement Score",
    ],
    modelMetrics: [
      { metric: "Hit Rate@k", explanation: "Ist das gekaufte Produkt in den Top-k Empfehlungen?" },
      { metric: "NDCG", explanation: "Qualit√§t des Rankings ‚Äì sind die besten Empfehlungen oben?" },
      { metric: "Diversity", explanation: "Werden verschiedene Kategorien empfohlen oder nur √§hnliche?" },
    ],
    metricsNote: "Offline-Metriken sind nur Proxies ‚Äì echter Erfolg zeigt sich nur im A/B-Test.",
    
    dataSources: [
      { source: "Transaktionen", features: ["Kaufhistorie", "Warenk√∂rbe", "Kauffrequenz"] },
      { source: "Browse-Verhalten", features: ["Angesehene Produkte", "Suchbegriffe", "Kategorie-Affinit√§t"] },
      { source: "Produktkatalog", features: ["Kategorien", "Attribute", "Preissegment", "Marke"] },
      { source: "Kontext", features: ["Tageszeit", "Device", "Wetter", "lokale Events"] },
    ],
    labelDefinition: "Positiv = Kunde hat empfohlenes Produkt gekauft (nicht nur geklickt).",
    
    pitfalls: [
      {
        title: "Popularity Bias",
        problem: "Modell empfiehlt immer Bestseller.",
        whyBad: "Keine echte Personalisierung, Long-Tail-Produkte werden ignoriert.",
        solution: "Popularity-Penalty oder Diversity-Constraints einbauen.",
      },
      {
        title: "Cold Start",
        problem: "Neue Kunden oder Produkte haben keine Historie.",
        whyBad: "Modell kann keine Empfehlung geben.",
        solution: "Fallback auf Content-based oder Popularity, explizite Pr√§ferenz-Abfrage.",
      },
      {
        title: "Feedback Loop",
        problem: "Nur empfohlene Produkte werden gekauft.",
        whyBad: "Modell best√§tigt sich selbst, Exploration fehlt.",
        solution: "Exploration-Anteil (z.B. 10% zuf√§llige Empfehlungen) einbauen.",
      },
      {
        title: "Offline-Online-Gap",
        problem: "Modell optimiert auf historische K√§ufe.",
        whyBad: "Vergangenes Kaufverhalten ‚â† aktuelle Intention.",
        solution: "Session-basierte Signale st√§rker gewichten, Online-Learning.",
      },
      {
        title: "Kontext ignoriert",
        problem: "Empfehlungen sind statisch f√ºr jeden Kunden.",
        whyBad: "Morgens und abends, mobil und desktop ‚Äì unterschiedliche Needs.",
        solution: "Kontextuelle Features (Zeit, Device, Situation) einbauen.",
      },
    ],
    
    practiceStory: {
      title: "Die Bestseller-Falle",
      situation: "Ein Einzelh√§ndler implementierte ein Empfehlungssystem, das Offline-Metriken um 30% verbesserte.",
      problem: "Im A/B-Test zeigte sich: Kein signifikanter Umsatzunterschied zur Baseline.",
      cause: "Das Modell empfahl haupts√§chlich Bestseller, die Kunden sowieso gefunden h√§tten. Die Offline-Metrik (Hit Rate) war hoch, weil Bestseller oft gekauft wurden ‚Äì aber nicht wegen der Empfehlung.",
      learning: "Offline-Metriken k√∂nnen t√§uschen. Nur A/B-Tests zeigen den echten inkrementellen Wert.",
    },
  },
  
  // ========== RISIKO & ANOMALIE (3) ==========
  {
    id: "fraud-insurance",
    emoji: "üîç",
    title: "Fraud Detection",
    industry: "insurance",
    problemType: "risk",
    level: "intermediate",
    levelStars: 2,
    shortDescription: "Erkenne betr√ºgerische Versicherungsanspr√ºche vor der Auszahlung.",
    
    goal: "Betrugsf√§lle automatisch identifizieren und zur manuellen Pr√ºfung eskalieren.",
    decision: "Welche Schadensmeldungen sollen von Fraud-Spezialisten gepr√ºft werden?",
    intervention: "Tiefenpr√ºfung, Gutachterbeauftragung, ggf. Ablehnung des Anspruchs.",
    baseline: "Stichprobenartige Pr√ºfung (~5%) oder regelbasierte Flags ‚Äì viele Betrugsf√§lle werden √ºbersehen.",
    
    businessKPIs: [
      "Fraud-Rate (erkannt)",
      "Schadenssumme verhindert",
      "False Positive Rate (Kunden√§rger)",
      "Pr√ºfungskosten pro Case",
    ],
    modelMetrics: [
      { metric: "Precision", explanation: "Wie viele der eskalierten Cases sind echte Betrugsf√§lle?" },
      { metric: "Recall", explanation: "Wie viele echte Betrugsf√§lle werden erkannt?" },
      { metric: "Savings/False Positive", explanation: "Trade-off zwischen Einsparung und Kundenbel√§stigung." },
    ],
    metricsNote: "Extrem unbalancierte Klassen (<1% Fraud) ‚Äì Standard-Accuracy ist nutzlos.",
    
    dataSources: [
      { source: "Schadenmeldung", features: ["Schadensart", "Schadensh√∂he", "Zeitpunkt", "Beschreibung"] },
      { source: "Vertragsdaten", features: ["Vertragsbeginn", "Deckungssumme", "Vorsch√§den", "Beitragsh√∂he"] },
      { source: "Kundendaten", features: ["Alter", "Region", "Beruf", "Vorversicherungen"] },
      { source: "Externe Daten", features: ["Wetter", "Polizeiberichte", "Werkstatt-Netzwerk"] },
    ],
    labelDefinition: "Fraud = Schaden wurde nach Pr√ºfung als betr√ºgerisch eingestuft und abgelehnt.",
    
    pitfalls: [
      {
        title: "Label-Qualit√§t",
        problem: "Nur erkannte Betrugsf√§lle sind gelabelt.",
        whyBad: "Unerkannte F√§lle werden als 'legitimate' trainiert.",
        solution: "Regelm√§√üige Tiefenpr√ºfung von Stichproben f√ºr bessere Labels.",
      },
      {
        title: "Discrimination-Risiko",
        problem: "Modell nutzt Postleitzahl als starkes Feature.",
        whyBad: "Kann zu unfairer Behandlung bestimmter Regionen f√ºhren.",
        solution: "Fairness-Audit, sensible Features pr√ºfen oder entfernen.",
      },
      {
        title: "Adversarial Attacks",
        problem: "Betr√ºger lernen die Modell-Regeln.",
        whyBad: "Muster √§ndern sich, Modell wird obsolet.",
        solution: "Regelm√§√üiges Retraining, Feature-Rotation, Ensemble-Methoden.",
      },
      {
        title: "Kundenerlebnis vs. Savings",
        problem: "Hoher Recall = viele False Positives.",
        whyBad: "Ehrliche Kunden werden frustriert, wenn Anspr√ºche verz√∂gert werden.",
        solution: "Tier-System: High-Confidence Cases automatisch, Rest gestaffelt pr√ºfen.",
      },
      {
        title: "Schaden erst nach Regulierung erkannt",
        problem: "Label wird erst Monate sp√§ter bekannt.",
        whyBad: "Modell-Feedback-Schleife ist sehr langsam.",
        solution: "Intermedi√§re Signale nutzen (Inkonsistenzen, Gutachter-Flags).",
      },
    ],
    
    practiceStory: {
      title: "Der Postleitzahlen-Bias",
      situation: "Eine Versicherung implementierte Fraud Detection mit 90% Recall auf dem Testset.",
      problem: "Nach Beschwerden zeigte sich: Kunden in bestimmten Stadtteilen wurden systematisch h√§ufiger gepr√ºft.",
      cause: "Historisch wurden in diesen Gebieten mehr Betrugsf√§lle aufgedeckt ‚Äì weil dort intensiver gepr√ºft wurde. Das Modell verst√§rkte diesen Bias.",
      learning: "Fairness-Audits sind Pflicht. Historische Pr√ºfungs-Intensit√§t ‚â† echte Fraud-Rate.",
    },
  },
  
  {
    id: "credit-risk",
    emoji: "üí≥",
    title: "Credit Risk",
    industry: "bank",
    problemType: "risk",
    level: "expert",
    levelStars: 3,
    shortDescription: "Bewerte das Ausfallrisiko von Kreditantr√§gen f√ºr bessere Kreditentscheidungen.",
    
    goal: "Risiko-adjustierte Kreditentscheidungen treffen und Ausfallraten minimieren.",
    decision: "Soll der Kreditantrag bewilligt werden? Zu welchen Konditionen?",
    intervention: "Ablehnung, Genehmigung, oder Genehmigung mit angepassten Konditionen (Zins, Sicherheiten).",
    baseline: "Regelbasiertes Scoring + manuelle Pr√ºfung ‚Äì langsam und nicht optimal.",
    
    businessKPIs: [
      "Default Rate (30/60/90 days)",
      "Expected Loss vs. Actual Loss",
      "Approval Rate",
      "Risk-adjusted Return on Capital",
    ],
    modelMetrics: [
      { metric: "Gini Coefficient", explanation: "Wie gut trennt das Modell gute von schlechten Krediten?" },
      { metric: "KS Statistic", explanation: "Maximale Trennung zwischen Default- und Non-Default-Verteilungen." },
      { metric: "Calibration", explanation: "Entspricht die vorhergesagte Wahrscheinlichkeit der echten Ausfallrate?" },
    ],
    metricsNote: "Regulatorische Anforderungen (BCBS, SR 11-7) erfordern spezifische Validierungen.",
    
    dataSources: [
      { source: "Antragsdaten", features: ["Einkommen", "Besch√§ftigung", "Kreditsumme", "Verwendungszweck"] },
      { source: "Schufa/Auskunfteien", features: ["Score", "Bonit√§tsmerkmale", "Anfragen", "Mahnverfahren"] },
      { source: "Kontodaten (PSD2)", features: ["Kontostand-Verlauf", "Einnahmen/Ausgaben", "Lastschrift-R√ºckl√§ufer"] },
      { source: "Interne Historie", features: ["Bestandskunde", "Produkt-Historie", "Zahlungsverhalten"] },
    ],
    labelDefinition: "Default = Kredit 90+ Tage √ºberf√§llig oder Insolvenz innerhalb von 12 Monaten.",
    
    pitfalls: [
      {
        title: "Selection Bias",
        problem: "Training nur auf bewilligten Krediten.",
        whyBad: "Abgelehnte Antr√§ge fehlen ‚Äì Modell sieht nur Teil der Population.",
        solution: "Reject Inference oder Testportfolio mit zuf√§lligen Bewilligungen.",
      },
      {
        title: "Vintage-Effekte",
        problem: "Modell trainiert auf einem Zeitraum, deployed in einem anderen.",
        whyBad: "Wirtschaftszyklen, Krisen, etc. √§ndern Ausfallmuster.",
        solution: "Out-of-time Validation, regelm√§√üiges Monitoring auf Drift.",
      },
      {
        title: "Interpretierbarkeit",
        problem: "Black-Box-Modell f√ºr regulierte Entscheidung.",
        whyBad: "Regulatorisch problematisch, Ablehnungsgr√ºnde nicht erkl√§rbar.",
        solution: "Interpretierbare Modelle (Scorecard) oder SHAP/LIME f√ºr Erkl√§rungen.",
      },
      {
        title: "Feature-Stabilit√§t",
        problem: "Volatile Features wie 'aktuelle Zinsen' als Input.",
        whyBad: "Modell-Performance schwankt mit externen Faktoren.",
        solution: "Stabile, kontrollierbare Features bevorzugen.",
      },
      {
        title: "Fairness-Anforderungen",
        problem: "Modell diskriminiert gesch√ºtzte Gruppen.",
        whyBad: "Rechtliche und Reputationsrisiken.",
        solution: "Fairness-Testing, Disparate Impact Analyse, ggf. Constraints.",
      },
    ],
    
    practiceStory: {
      title: "Die Pandemie-√úberraschung",
      situation: "Eine Bank hatte ein Credit-Risk-Modell mit exzellenter Performance √ºber 5 Jahre.",
      problem: "2020 explodierten die Ausfallraten ‚Äì das Modell hatte v√∂llig versagt.",
      cause: "Das Modell war auf stabilen Wirtschaftsdaten trainiert. Die Corona-Krise war 'Out of Distribution' ‚Äì Branchenzugeh√∂rigkeit (Gastro, Tourismus) war pl√∂tzlich der wichtigste Faktor.",
      learning: "Stresstests und Szenario-Analysen sind kritisch. Modelle k√∂nnen bei Regime-Wechseln versagen.",
    },
  },
  
  {
    id: "transaction-monitoring",
    emoji: "üîê",
    title: "Transaction Monitoring",
    industry: "fintech",
    problemType: "risk",
    level: "expert",
    levelStars: 3,
    shortDescription: "Erkenne verd√§chtige Transaktionen in Echtzeit f√ºr AML- und Fraud-Pr√§vention.",
    
    goal: "Verd√§chtige Transaktionsmuster identifizieren und zur Pr√ºfung eskalieren.",
    decision: "Soll die Transaktion blockiert, verz√∂gert oder zur Pr√ºfung markiert werden?",
    intervention: "Transaktion stoppen, 2FA anfordern, manueller Review, SAR-Meldung.",
    baseline: "Regelbasiertes Alerting mit hoher False-Positive-Rate (>95%).",
    
    businessKPIs: [
      "False Positive Rate",
      "Alert-to-SAR Ratio",
      "Fraud Losses",
      "Analyst Workload",
    ],
    modelMetrics: [
      { metric: "Precision@Alert-Volume", explanation: "Bei fixem Alert-Budget: Wie viele echte Cases?" },
      { metric: "Detection Latency", explanation: "Wie schnell nach der Transaktion kommt der Alert?" },
      { metric: "Pattern Coverage", explanation: "Werden verschiedene Fraud-Typen erkannt?" },
    ],
    metricsNote: "Echtzeit-Anforderungen erfordern Trade-off zwischen Modellkomplexit√§t und Latenz.",
    
    dataSources: [
      { source: "Transaktionen", features: ["Betrag", "Zeitpunkt", "Empf√§nger", "Kanal", "Device"] },
      { source: "Kundenprofil", features: ["√úbliches Verhalten", "Risikoklasse", "Verifizierungsstatus"] },
      { source: "Netzwerk", features: ["Bekannte Fraud-Accounts", "Ring-Transaktionen", "Neue Verbindungen"] },
      { source: "Device/IP", features: ["Geolocation", "Device-Fingerprint", "IP-Reputation"] },
    ],
    labelDefinition: "Fraud = Transaktion wurde als betr√ºgerisch best√§tigt (durch Kunden oder Ermittlung).",
    
    pitfalls: [
      {
        title: "Latenz-Constraints",
        problem: "Komplexe Modelle sind zu langsam f√ºr Echtzeit.",
        whyBad: "Transaktion muss in <100ms entschieden werden.",
        solution: "Zwei-Stufen-System: Schnelle Vorfilterung + asynchrone Deep Analysis.",
      },
      {
        title: "Label-Feedback-Loop",
        problem: "Blockierte Transaktionen k√∂nnen nicht verifiziert werden.",
        whyBad: "War es wirklich Fraud? Ohne Feedback kein Lernen.",
        solution: "Stichproben durchlassen, Kunden-R√ºckfragen systematisch erfassen.",
      },
      {
        title: "Evolving Patterns",
        problem: "Betr√ºger passen sich an.",
        whyBad: "Modell-Performance degradiert schnell.",
        solution: "Kontinuierliches Monitoring, schnelles Retraining, Adversarial Testing.",
      },
      {
        title: "Alert Fatigue",
        problem: "Analysten m√ºssen tausende Alerts pro Tag bearbeiten.",
        whyBad: "Echte Cases werden √ºbersehen, Burnout.",
        solution: "Precision optimieren, intelligentes Alert-Batching, Priorisierung.",
      },
      {
        title: "Regulatorische Anforderungen",
        problem: "AML-Compliance erfordert bestimmte Pr√ºfungen.",
        whyBad: "ML-Modell kann regulatorische Regeln nicht einfach ersetzen.",
        solution: "Hybrid: Regelwerk f√ºr Compliance + ML f√ºr Effizienz.",
      },
    ],
    
    practiceStory: {
      title: "Die Alert-Flut",
      situation: "Ein Fintech implementierte ML-basiertes Transaction Monitoring und reduzierte False Positives um 60%.",
      problem: "Die Regulierungsbeh√∂rde bem√§ngelte: Wichtige SAR-F√§lle wurden vom Modell niedriger priorisiert.",
      cause: "Das Modell war auf historische Fraud-Best√§tigungen trainiert. SAR-pflichtige Geldw√§sche-Muster waren anders als Fraud-Muster ‚Äì und wurden vom Modell nicht gelernt.",
      learning: "AML und Fraud sind verschiedene Use Cases. Regulatorische Anforderungen m√ºssen separat abgebildet werden.",
    },
  },
  
  // ========== NACHFRAGE & MENGE (2) ==========
  {
    id: "demand-forecasting",
    emoji: "üìä",
    title: "Demand Forecasting",
    industry: "retail",
    problemType: "demand",
    level: "intermediate",
    levelStars: 2,
    shortDescription: "Prognostiziere die Nachfrage nach Produkten f√ºr optimale Bestandsplanung.",
    
    goal: "Nachfrage pro Produkt und Standort vorhersagen f√ºr bessere Disposition.",
    decision: "Wie viel von welchem Produkt soll bestellt/produziert werden?",
    intervention: "Automatische Bestellvorschl√§ge, Produktions-Planung, Filial-Allokation.",
    baseline: "Manuelle Planung basierend auf Vorjahr + Bauchgef√ºhl ‚Äì hohe Stockout- und √úberbestand-Raten.",
    
    businessKPIs: [
      "Forecast Accuracy (WMAPE)",
      "Stockout-Rate",
      "√úberbestand-Rate",
      "Inventory Turnover",
    ],
    modelMetrics: [
      { metric: "WMAPE", explanation: "Weighted Mean Absolute Percentage Error ‚Äì ber√ºcksichtigt Umsatzgewichtung." },
      { metric: "Bias", explanation: "Systematische √úber-/Untersch√§tzung erkennen." },
      { metric: "Service Level", explanation: "Prozent der Nachfrage, die erf√ºllt werden kann." },
    ],
    metricsNote: "Forecast-Fehler bei langsam drehenden Artikeln (Long Tail) sind normal h√∂her.",
    
    dataSources: [
      { source: "POS-Daten", features: ["Historische Verk√§ufe", "Preise", "Promotions", "Kannibalisierung"] },
      { source: "Stammdaten", features: ["Kategorie", "Saisonalit√§t", "Lifecycle-Status", "Substitutes"] },
      { source: "Externe Daten", features: ["Wetter", "Feiertage", "Events", "Wirtschaftsindikatoren"] },
      { source: "Supply Chain", features: ["Lieferzeiten", "Mindestbestellmengen", "Lagerkapazit√§t"] },
    ],
    labelDefinition: "Zielgr√∂√üe = Tats√§chliche Verk√§ufe (bereinigt um Stockouts, wenn m√∂glich).",
    
    pitfalls: [
      {
        title: "Stockout-Verzerrung",
        problem: "Verk√§ufe = 0, weil Produkt nicht verf√ºgbar war.",
        whyBad: "Modell lernt f√§lschlich 'keine Nachfrage'.",
        solution: "Stockouts erkennen und Nachfrage sch√§tzen (Lost Sales).",
      },
      {
        title: "Promotion-Effekte",
        problem: "Historische Promotions nicht als Feature.",
        whyBad: "Modell verwechselt Promotion-Peaks mit normaler Nachfrage.",
        solution: "Promotion-Kalender als Feature, separate Baseline + Uplift.",
      },
      {
        title: "Neue Produkte",
        problem: "Keine Historie f√ºr Neueinf√ºhrungen.",
        whyBad: "Cold-Start-Problem, keine Prognose m√∂glich.",
        solution: "Analogie-Methoden, Attribute-basierte Sch√§tzung, kurze Testphase.",
      },
      {
        title: "Aggregationslevel",
        problem: "Forecast auf falscher Granularit√§t.",
        whyBad: "Zu granular = hohes Rauschen, zu aggregiert = keine Filial-Steuerung.",
        solution: "Hierarchisches Forecasting: Bottom-up und Top-down kombinieren.",
      },
      {
        title: "Kannibalisierung ignoriert",
        problem: "Promo-Produkt A steigert, aber Produkt B sinkt.",
        whyBad: "Gesamteffekt wird √ºbersch√§tzt.",
        solution: "Cross-Elastizit√§ten modellieren, Kategorie-Level betrachten.",
      },
    ],
    
    practiceStory: {
      title: "Der Sommer, der keiner war",
      situation: "Ein Einzelh√§ndler trainierte Demand Forecasting auf 3 Jahren Historie mit gutem WMAPE.",
      problem: "Im verregneten Sommer 2021 waren Grillprodukte massiv √ºberbestellt.",
      cause: "Das Modell hatte 'Sommer' als Feature, aber keine Wetterdaten. Historisch war Sommer = Grillsaison. Das ungew√∂hnliche Wetter konnte nicht antizipiert werden.",
      learning: "Externe Faktoren (Wetter) als Features einbauen ‚Äì und die Grenzen der Vorhersagbarkeit akzeptieren.",
    },
  },
  
  {
    id: "inventory-optimization",
    emoji: "üì¶",
    title: "Inventory Optimization",
    industry: "ecommerce",
    problemType: "demand",
    level: "intermediate",
    levelStars: 2,
    shortDescription: "Optimiere Lagerbest√§nde √ºber alle Produkte und Standorte hinweg.",
    
    goal: "Optimale Bestandsh√∂he pro SKU und Lager f√ºr minimale Kosten bei hohem Service Level.",
    decision: "Wann und wie viel soll nachbestellt werden? Wo soll gelagert werden?",
    intervention: "Automatische Bestellausl√∂sung, Lagerort-Empfehlungen, Markdown-Timing.",
    baseline: "Fixe Mindestbest√§nde und Bestellmengen ‚Äì oft zu hoch oder zu niedrig.",
    
    businessKPIs: [
      "Inventory Holding Costs",
      "Stockout Rate",
      "Days of Supply",
      "Obsolescence Rate",
    ],
    modelMetrics: [
      { metric: "Service Level", explanation: "Prozent der Nachfrage, die sofort erf√ºllt werden kann." },
      { metric: "Fill Rate", explanation: "Prozent der Bestellpositionen, die komplett geliefert werden." },
      { metric: "Inventory Turnover", explanation: "Wie oft dreht sich der Bestand pro Jahr?" },
    ],
    metricsNote: "Optimierung ist Multi-Objective: Service Level vs. Kosten.",
    
    dataSources: [
      { source: "Demand Forecast", features: ["Erwartete Nachfrage", "Forecast-Unsicherheit"] },
      { source: "Supply Chain", features: ["Lieferzeiten", "Mindestmengen", "Kosten pro Einheit"] },
      { source: "Bestandsdaten", features: ["Aktueller Bestand", "Reservierungen", "Transit-Bestand"] },
      { source: "Finanzdaten", features: ["Lagerkosten", "Kapitalbindung", "Abschreibungsregeln"] },
    ],
    labelDefinition: "Optimierung auf Kostenfunktion: Lagerhaltungskosten + Stockout-Kosten + Handling-Kosten.",
    
    pitfalls: [
      {
        title: "Lokale Optimierung",
        problem: "Jede Filiale optimiert unabh√§ngig.",
        whyBad: "Gesamtbestand suboptimal, Verschiebungen zwischen Lagern ignoriert.",
        solution: "Netzwerk-Optimierung, zentrale Koordination.",
      },
      {
        title: "Stockout-Kosten falsch gesch√§tzt",
        problem: "Stockout = nur entgangener Umsatz.",
        whyBad: "Kundenabwanderung, Reputationsschaden werden ignoriert.",
        solution: "Langfristige Effekte und Customer Lifetime Value einbeziehen.",
      },
      {
        title: "Lieferzeit-Variabilit√§t",
        problem: "Feste Lieferzeit im Modell.",
        whyBad: "Realit√§t: Lieferzeiten schwanken, Sicherheitsbestand zu niedrig.",
        solution: "Stochastische Lieferzeiten modellieren, Sicherheitsbestand anpassen.",
      },
      {
        title: "Produkt-Lebenszyklen",
        problem: "Modell behandelt alle Produkte gleich.",
        whyBad: "Neue Produkte brauchen anderen Bestand als Auslauf-Artikel.",
        solution: "Lifecycle-Phase als Feature, unterschiedliche Strategien.",
      },
      {
        title: "Discount-Timing",
        problem: "√úberbestand wird zu sp√§t abgewertet.",
        whyBad: "H√∂here Abschreibungen, wenn Saison vorbei ist.",
        solution: "Proaktives Markdown-Modell, das Timing optimiert.",
      },
    ],
    
    practiceStory: {
      title: "Das gef√ºllte Lager",
      situation: "Ein E-Commerce-H√§ndler optimierte Best√§nde mit einem ML-Modell und reduzierte Stockouts um 40%.",
      problem: "Die Lagerkosten stiegen im selben Zeitraum um 60%.",
      cause: "Das Modell optimierte nur auf Service Level (Stockout-Vermeidung). Die Lagerhaltungskosten waren nicht Teil der Zielfunktion ‚Äì das Modell bestellte 'sicherheitshalber' zu viel.",
      learning: "Multi-Objective-Optimierung: Service Level UND Kosten m√ºssen in der Zielfunktion sein.",
    },
  },
  
  // ========== AUSFALL & WARTUNG (1) ==========
  {
    id: "predictive-maintenance",
    emoji: "‚öôÔ∏è",
    title: "Predictive Maintenance",
    industry: "manufacturing",
    problemType: "maintenance",
    level: "expert",
    levelStars: 3,
    shortDescription: "Sage Maschinenausf√§lle vorher, um Wartung optimal zu planen.",
    
    goal: "Ungeplante Stillst√§nde vermeiden durch vorausschauende Wartung.",
    decision: "Wann soll welche Komponente gewartet oder ausgetauscht werden?",
    intervention: "Geplante Wartung w√§hrend Produktionspausen, Ersatzteil-Bestellung.",
    baseline: "Pr√§ventive Wartung nach Zeitplan ‚Äì oft zu fr√ºh (teuer) oder zu sp√§t (Ausfall).",
    
    businessKPIs: [
      "Ungeplante Stillstandszeit",
      "Wartungskosten",
      "OEE (Overall Equipment Effectiveness)",
      "Ersatzteil-Lagerkosten",
    ],
    modelMetrics: [
      { metric: "Precision", explanation: "Wie oft tritt der vorhergesagte Ausfall tats√§chlich ein?" },
      { metric: "Lead Time", explanation: "Wie viel Vorlauf gibt das Modell f√ºr Wartungsplanung?" },
      { metric: "RUL Accuracy", explanation: "Remaining Useful Life ‚Äì wie genau ist die Restlebensdauer-Sch√§tzung?" },
    ],
    metricsNote: "Trade-off: Zu fr√ºhe Warnung = unn√∂tige Wartung, zu sp√§te Warnung = Ausfall.",
    
    dataSources: [
      { source: "Sensordaten", features: ["Vibration", "Temperatur", "Druck", "Stromaufnahme", "Ger√§usche"] },
      { source: "Wartungshistorie", features: ["Vergangene Ausf√§lle", "Reparaturen", "Ersatzteile"] },
      { source: "Betriebsdaten", features: ["Laufzeit", "Last", "Produktionszyklen", "Bediener"] },
      { source: "Umgebungsdaten", features: ["Temperatur", "Luftfeuchtigkeit", "Staub"] },
    ],
    labelDefinition: "Ausfall = Komponente ist ausgefallen oder hat kritischen Schwellenwert unterschritten.",
    
    pitfalls: [
      {
        title: "Survival Bias",
        problem: "Nur Daten von Maschinen, die gewartet wurden.",
        whyBad: "Ausfall-Muster von ungewarteten Maschinen fehlen.",
        solution: "Run-to-Failure-Tests an ausgew√§hlten Komponenten.",
      },
      {
        title: "Sensor-Anomalien ‚â† Ausf√§lle",
        problem: "Modell erkennt Sensor-Fehler als Maschinenprobleme.",
        whyBad: "False Positives durch defekte Sensoren.",
        solution: "Sensor-Health-Check voranstellen, Multi-Sensor-Korrelation.",
      },
      {
        title: "Dom√§nenwissen ignoriert",
        problem: "Pure ML ohne Verst√§ndnis der Physik.",
        whyBad: "Modell lernt Korrelationen, nicht Kausalit√§ten.",
        solution: "Physics-informed ML, Zusammenarbeit mit Ingenieuren.",
      },
      {
        title: "Datenqualit√§t in der Fabrik",
        problem: "Sensor-Ausf√§lle, manuelle Eingriffe, unvollst√§ndige Logs.",
        whyBad: "Rauschen und L√ºcken in den Trainingsdaten.",
        solution: "Robuste Modelle, Datenqualit√§ts-Monitoring, Edge-Processing.",
      },
      {
        title: "Integration in Prozesse",
        problem: "Modell warnt, aber Wartung reagiert nicht.",
        whyBad: "Keine Verbindung zwischen Prediction und Aktion.",
        solution: "Integration in CMMS, automatische Work-Order-Erstellung.",
      },
    ],
    
    practiceStory: {
      title: "Der ignorierten Warnung",
      situation: "Ein Fertigungsunternehmen implementierte Predictive Maintenance mit 85% Recall auf Ausf√§lle.",
      problem: "Die ungeplanten Stillst√§nde blieben auf dem gleichen Niveau.",
      cause: "Das Modell warnte korrekt, aber die Warnungen landeten in E-Mails, die niemand las. Die Integration in den Wartungsprozess fehlte v√∂llig ‚Äì es gab keine automatischen Work Orders.",
      learning: "Ein Modell ist nur so gut wie seine Integration in den operativen Prozess. Warnung ohne Aktion ist nutzlos.",
    },
  },
];
